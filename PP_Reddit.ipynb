{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sb\n",
    "import pandas as pd\n",
    "import praw\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import os\n",
    "import nltk\n",
    "style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 7.0.0 of praw is outdated. Version 7.1.0 was released Tuesday June 23, 2020.\n"
     ]
    }
   ],
   "source": [
    "r = praw.Reddit(client_id='bvqtjKrg-4OJzA', \n",
    "                     client_secret='aNb_AKNWjcidfCftt85u86VUnsNpOw', \n",
    "                     user_agent='SCRIPT_NAME', \n",
    "                     username='peer_max_alon', \n",
    "                     password='peerproduction1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subreddit distribution functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mods(subreddit):\n",
    "    bots = ['AutoModerator', 'modlog_research_bot', 'PoliticsModeratorBot', 'rGameMods']\n",
    "    mods = [mod.name for mod in subreddit.moderator() if mod.name not in bots]\n",
    "    return mods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posts(mods, sub, sub_name):\n",
    "    for mod in mods:\n",
    "        posts_list = []\n",
    "        posts = r.redditor(mod).submissions.new(limit=500)\n",
    "        for post in posts:\n",
    "            d = {}\n",
    "            d['subreddit'] = post.subreddit.display_name\n",
    "            d['title'] = post.title\n",
    "#           d['url'] = \"https://www.reddit.com\" + post.permalink\n",
    "            posts_list.append(d)\n",
    "        df = pd.DataFrame(posts_list)\n",
    "        df.to_csv(f'data/{sub_name}/posts/{mod}.csv', index=False)\n",
    "    distribution_aggregator(mods, sub_name, 'posts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments(mods, sub, sub_name):\n",
    "    for mod in mods:\n",
    "        comments_list = []\n",
    "        comments = r.redditor(mod).comments.new(limit=500)\n",
    "        for comment in comments:\n",
    "            d = {}\n",
    "            d['subreddit'] = comment.subreddit.display_name\n",
    "            d['body'] = comment.body\n",
    "#           d['url'] = \"https://www.reddit.com\" + comment.submission.permalink\n",
    "            comments_list.append(d)\n",
    "        df = pd.DataFrame(comments_list)\n",
    "        df.to_csv(f'data/{sub_name}/comments/{mod}.csv', index=False)\n",
    "    distribution_aggregator(mods, sub_name, 'comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution_aggregator(mods, sub_name, submission_type):\n",
    "    total = []\n",
    "    for mod in mods:\n",
    "        try:\n",
    "            data = pd.read_csv(f'data/{sub_name}/{submission_type}/{mod}.csv')\n",
    "        except pd.errors.EmptyDataError:\n",
    "            total.append(dict())\n",
    "            continue\n",
    "        counter = dict(Counter(data['subreddit']))\n",
    "        total.append(counter)\n",
    "    distribution = pd.DataFrame(total, index=mods, dtype=np.int)\n",
    "    distribution.fillna(0,inplace=True)\n",
    "    distribution[distribution.columns] = distribution[distribution.columns].astype(int)\n",
    "    distribution.to_excel(f'data/{sub_name}/{submission_type}/{submission_type}_aggreagated.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_aggregator(mods, sub_name, submission_type, col_name):\n",
    "    all_values = []\n",
    "    for mod in mods:\n",
    "        data = pd.read_csv(f'data/{sub_name}/{submission_type}/{mod}.csv')\n",
    "        all_values.extend(data[col_name].tolist())\n",
    "    return all_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_freq_counter(normalized_arrays, sub_name, submission_type):\n",
    "    word_freq = Counter()\n",
    "    for token_array in normalized_arrays:\n",
    "        word_freq = word_freq + Counter(token_array)\n",
    "    most_common = word_freq.most_common()[0:140]\n",
    "    df_word_freq = pd.DataFrame(most_common, columns=[\"word\", \"count\"]).set_index('word')\n",
    "    file_name = 'posts_title' if submission_type=='posts' else submission_type\n",
    "    df_word_freq.to_excel(f'data/{sub_name}/{submission_type}/{file_name}_wordfreq.xlsx')\n",
    "    return most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(tokens):\n",
    "    pos_tagged = pos_tag(tokens)\n",
    "    wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [] \n",
    "    for word, tag in wordnet_tagged: \n",
    "        if tag is None: \n",
    "            lemmatized.append(word) \n",
    "        else:         \n",
    "            lemmatized.append(lemmatizer.lemmatize(word, tag)) \n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(lines):\n",
    "    tk = RegexpTokenizer(r'\\b[^\\d\\W]+\\b') # only words without numbers\n",
    "    tokenized_arrays = [tk.tokenize(line) for line in lines]\n",
    "    return tokenized_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizer(tokens_arrays):\n",
    "    normalized = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for tokens_array in tokens_arrays:\n",
    "        lowercase = [w.lower() for w in tokens_array if 2 < len(w) < 12] # lowercase tokenized sentence\n",
    "        filtered_stopwords = [w.lower() for w in lowercase if not w in stop_words] # filter stopwords \n",
    "        normalized.append(lemmatization(filtered_stopwords))\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_cloud(counter, sub_name, submission_type):\n",
    "    wordcloud = WordCloud(\n",
    "        width=1024, height=1024, background_color='white', min_font_size=10\n",
    "    ).generate_from_frequencies(counter)\n",
    "    plt.figure(figsize=(10, 10), facecolor=None)\n",
    "    plt.imshow(wordcloud)\n",
    "    file_name = f'posts titles' if submission_type=='posts' else 'comments'\n",
    "    plt.title(f'{sub_name} - {file_name} of moderators - word cloud', fontsize=18)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.savefig(f'data/{sub_name}/{submission_type}/word_cloud.png',dpi=100)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def folder_creation(sub_name):\n",
    "    if os.path.isdir(f'data/{sub_name}'):\n",
    "        return\n",
    "    else:\n",
    "        os.mkdir(f'data/{sub_name}')\n",
    "        os.mkdir(f'data/{sub_name}/posts')\n",
    "        os.mkdir(f'data/{sub_name}/comments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#['politics', 'games', 'askdocs']\n",
    "sub_names = ['gameofthrones']\n",
    "\n",
    "for sub_name in sub_names:\n",
    "   \n",
    "    # create folder structure\n",
    "    folder_creation(sub_name)\n",
    "    \n",
    "    # get mods\n",
    "    sub = r.subreddit(sub_name)\n",
    "    mods = get_mods(sub)\n",
    "    \n",
    "    # agaggreate posts and comments\n",
    "    get_posts(mods, sub, sub_name)    \n",
    "    get_comments(mods, sub, sub_name)\n",
    "    \n",
    "    # generate word frequency\n",
    "    submissions = [('posts', 'title'), ('comments', 'body')]\n",
    "    for submission in submissions:\n",
    "        submission_type, col_name = (submission[0], submission[1])\n",
    "        all_values = column_aggregator(mods, sub_name, submission_type, col_name)\n",
    "        tokenized = tokenizer(all_values)\n",
    "        normalized = normalizer(tokenized)\n",
    "        word_freq = word_freq_counter(normalized, sub_name, submission_type)\n",
    "\n",
    "        # generate word cloud\n",
    "        generate_word_cloud(dict(word_freq), sub_name, submission_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
